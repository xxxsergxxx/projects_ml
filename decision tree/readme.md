# ğŸ§  Credit Risk Classification Using Decision Tree & Logistic Regression

This project focuses on building classification models to predict **credit risk** using the [UCI Credit Approval Dataset](https://archive.ics.uci.edu/ml/datasets/Credit+Approval). Two models are trained and compared:
- ğŸŒ³ `DecisionTreeClassifier`
- ğŸ“ˆ `LogisticRegression`

---

## ğŸ“‚ Dataset Overview

- ğŸ—‚ **File**: `crx.data`
- ğŸ“ **Rows**: 690
- ğŸ“Š **Features**: 15
- ğŸ¯ **Target**: A16 (credit approval `+` or `-`)

---

## ğŸ“Š EDA & Data Cleaning

### ğŸ” Issues Identified:
- Missing values marked with `'?'`
- Zero values and suspicious placeholders like `"00000"` in numeric features (`A14`, `A15`)
- Mixed data types â€” categorical features stored as `object`

### âœ… Actions Taken:
- Dropped rows with `'?'` (only 37 rows)
- Converted relevant features (`A2`, `A14`, `A15`) to numeric
- Filtered out outliers:  
  - `A14` capped at 2000  
  - `A15` capped at 22000

---

## ğŸ› ï¸ Feature Engineering

- ğŸ§® `A14_2 = A14 / A2`
- ğŸ§® `A8_3 = A8 / A3`
- ğŸ§® `A15_sqr = A15 ** 2`

---

## ğŸ§ª Train-Test-Validation Split

- ğŸ“š 90% used for training/testing
- âœ… 10% used for final validation
- From training data: 80% train, 20% test

---

## ğŸ” Preprocessing Pipeline

- ğŸ§¼ Categorical encoding via `TargetEncoder`
- ğŸ”¢ Scaling via `StandardScaler`
- ğŸ”„ Used `make_column_transformer` + `Pipeline`

---

## ğŸŒ³ Model 1: Decision Tree Classifier

### ğŸ”§ Hyperparameters Tuned:
- `max_depth`: [5, 10]
- `max_leaf_nodes`: [3, 6]

### âœ… Results:

#### Test Set:
Accuracy: 91% Precision: 0.89â€“0.93 Recall: 0.88â€“0.93


#### Validation Set:
Accuracy: 83% Class 0 F1: 0.84 Class 1 F1: 0.83
## ğŸ“ˆ Model 2: Logistic Regression

### ğŸ”§ Hyperparameters Tuned:
- `max_iter`: [100â€“900]
- `penalty`: `l2`

### âœ… Results:

#### Test Set:

Accuracy: 91% Precision: 0.90â€“0.91 Recall: 0.90â€“0.91


#### Validation Set:

Accuracy: 82% Class 0 F1: 0.82 Class 1 F1: 0.81


---

## ğŸ” Comparison

| Metric           | Decision Tree | Logistic Regression |
|------------------|----------------|----------------------|
| Cross-val Score  | **0.856**      | **0.861**            |
| Test Accuracy    | **91%**        | **91%**              |
| Valid Accuracy   | **83%**        | 82%                  |
| Best Use Case    | Slightly better generalization with trees |

âœ… **Conclusion**: Both models perform very similarly, with decision tree slightly outperforming on validation, potentially due to its ability to model non-linearity.

---

## ğŸ“¦ Base Model (No Feature Eng.)

| Metric             | Value          |
|--------------------|----------------|
| Cross-Val (Tree)   | 0.853          |
| Test Accuracy      | 83%            |

ğŸ” Feature engineering **improved model accuracy** slightly from 83% â¡ï¸ 91%

---

## ğŸš€ Installation

```bash
pip install scikit-learn pandas matplotlib seaborn category_encoders
```

##ğŸ§‘â€ğŸ’» Author

Serhii Kolotukhin

ğŸ“ www.linkedin.com/in/serhii-kolotuhkin-25648a166 | GitHub
